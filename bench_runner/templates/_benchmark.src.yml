---
name: _benchmark

"on":
  workflow_call:
    inputs:
      fork:
        description: "Fork of cpython to benchmark"
        type: string
      ref:
        description: "Branch, tag or (full) SHA commit to benchmark"
        type: string
      machine:
        description: "Machine to run on"
        type: string
      benchmarks:
        description: "Benchmarks to run (comma-separated; empty runs all benchmarks)"
        type: string
      pgo:
        description: "Build with PGO"
        type: boolean
      dry_run:
        description: "Dry run: Do not commit to the repo"
        type: boolean
      force:
        description: "Rerun and replace results if commit already exists"
        type: boolean
      perf:
        description: "Collect Linux perf profiling data (Linux only)"
        type: boolean
      tier2:
        description: "Enable the Tier 2 interpreter"
        type: boolean
      jit:
        description: "Enable the JIT"
        type: boolean

  workflow_dispatch:
    inputs:
      fork:
        description: "Fork of cpython to benchmark"
        type: string
        default: "python"
      ref:
        description: "Branch, tag or (full) SHA commit to benchmark"
        type: string
        default: "main"
      machine:
        description: "Machine to run on"
        default: "linux-amd64"
        type: choice
        options:
          - linux-amd64
          - windows-amd64
          - darwin-arm64
          - all
      benchmarks:
        description: "Benchmarks to run (comma-separated; empty runs all benchmarks)"
        type: string
      pgo:
        description: "Build with PGO"
        type: boolean
      dry_run:
        description: "Dry run: Do not commit to the repo"
        type: boolean
      force:
        description: "Rerun and replace results if commit already exists"
        type: boolean
      perf:
        description: "Collect Linux perf profiling data (Linux only)"
        type: boolean
      tier2:
        description: "Enable the Tier 2 interpreter"
        type: boolean
      jit:
        description: "Enable the JIT"
        type: boolean

jobs:
  benchmark-windows:
    runs-on: [self-hosted, windows, bare-metal]

    steps:
      # Tell git to checkout repos with symlinks (required by pyston
      # benchmarks).
      # Requires "Developer Mode" switched on in Windows 10/11
      - name: Enable symlinks for git
        run: |
          git config --global core.symlinks true
      - name: Checkout benchmarking
        uses: actions/checkout@v4
      - name: Checkout CPython
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.fork }}/cpython
          path: cpython
          ref: ${{ inputs.ref }}
          fetch-depth: 50
      - name: Install dependencies from PyPI
        run: |
          py -m venv venv
          venv\Scripts\python.exe -m pip install -r requirements.txt
      - name: Should we run?
        if: ${{ always() }}
        id: should_run
        run: |
          venv\Scripts\python.exe -m bench_runner should_run ${{ inputs.force }} ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} false ${{ inputs.tier2 }} ${{ inputs.jit }} >> $GITHUB_OUTPUT
      - name: Checkout python-macrobenchmarks
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: pyston/python-macrobenchmarks
          path: pyston-benchmarks
          ref: ${{ env.PYSTON_BENCHMARKS_HASH }}
      - name: Checkout pyperformance
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: python/pyperformance
          path: pyperformance
          ref: ${{ env.PYPERFORMANCE_HASH }}
      - name: Build Python
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          cd cpython
          PCbuild\build.bat ($env:BUILD_FLAGS -split ' ') ${{ inputs.pgo == true && '--pgo' || '' }} ${{ inputs.jit == true && '--experimental-jit' || '' }} ${{ inputs.tier2 == true && '--experimental-jit-interpreter' || '' }} -c Release
          # Copy the build products to a place that libraries can find them.
          Copy-Item -Path $env:BUILD_DEST -Destination "libs" -Recurse
      - name: Install pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          venv\Scripts\python.exe -m pip install --no-binary :all: .\pyperformance
      - name: Running pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          venv\Scripts\python.exe -m bench_runner run_benchmarks benchmark cpython\$env:BUILD_DEST\python.exe ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.benchmarks || 'all' }} --run_id ${{ github.run_id }} ${{ inputs.tier2 == true && '--flag PYTHON_UOPS' || '' }} ${{ inputs.jit == true && '--flag JIT' || '' }}
      # Pull again, since another job may have committed results in the meantime
      - name: Pull benchmarking
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run }}
        run: |
          # Another benchmarking task may have created results for the same
          # commit while the above was running. This "magic" incantation means
          # that any local results for this commit will override anything we
          # just pulled in in that case.
          git pull -s recursive -X ours --autostash --rebase
      - name: Add data to repo
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run }}
        uses: EndBug/add-and-commit@v9
        with:
          add: results
      - name: Upload artifacts
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark
          path: |
            benchmark.json
          overwrite: true

  benchmark-linux:
    runs-on: [self-hosted, linux, bare-metal]
    timeout-minutes: 1440

    steps:
      - name: Checkout benchmarking
        uses: actions/checkout@v4
      - uses: fregante/setup-git-user@v2
      - name: Setup system Python
        if: ${{ runner.arch == 'X64' }}
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
      - name: Checkout CPython
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.fork }}/cpython
          path: cpython
          ref: ${{ inputs.ref }}
          fetch-depth: 50
      - name: Install dependencies from PyPI
        run: |
          python -m venv venv
          venv/bin/python -m pip install -r requirements.txt
      - name: Should we run?
        if: ${{ always() }}
        id: should_run
        run: |
          venv/bin/python -m bench_runner should_run ${{ inputs.force }} ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.machine }} false ${{ inputs.tier2 }} ${{ inputs.jit }} >> $GITHUB_OUTPUT
      - name: Checkout python-macrobenchmarks
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: pyston/python-macrobenchmarks
          path: pyston-benchmarks
          ref: ${{ env.PYSTON_BENCHMARKS_HASH }}
      - name: Checkout pyperformance
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: python/pyperformance
          path: pyperformance
          ref: ${{ env.PYPERFORMANCE_HASH }}
      - name: Build Python
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          cd cpython
          ./configure ${{ inputs.pgo == true && '--enable-optimizations --with-lto=yes' || '' }} ${{ inputs.tier2 == true && '--enable-experimental-jit=interpreter' || '' }} ${{ inputs.jit == true && '--enable-experimental-jit=yes' || '' }}
          make ${{ runner.arch == 'ARM64' && '-j' || '-j4' }}
      - name: Install pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          venv/bin/python -m pip install --no-binary :all: ./pyperformance
      - name: Tune system
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          sudo LD_LIBRARY_PATH=$LD_LIBRARY_PATH venv/bin/python -m pyperf system tune
      - name: Tune for (Linux) perf
        if: ${{ steps.should_run.outputs.should_run != 'false' && inputs.perf }}
        run: |
          sudo bash -c "echo 100000 > /proc/sys/kernel/perf_event_max_sample_rate"
      - name: Running pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          rm -rf ~/.debug/*
          venv/bin/python -m bench_runner run_benchmarks ${{ inputs.perf && 'perf' || 'benchmark' }} cpython/python ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.benchmarks || 'all' }} --run_id ${{ github.run_id }} ${{ inputs.tier2 == true && '--flag PYTHON_UOPS' || '' }} ${{ inputs.jit == true && '--flag JIT' || '' }}
      # Pull again, since another job may have committed results in the meantime
      - name: Pull benchmarking
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run && !inputs.perf }}
        run: |
          # Another benchmarking task may have created results for the same
          # commit while the above was running. This "magic" incantation means
          # that any local results for this commit will override anything we
          # just pulled in in that case.
          git pull -s recursive -X ours --autostash --rebase
      - name: Adding data to repo
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run && !inputs.perf }}
        uses: EndBug/add-and-commit@v9
        with:
          add: results
      - name: Upload benchmark artifacts
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.perf }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark
          path: |
            benchmark.json
          overwrite: true
      - name: Upload perf artifacts
        if: ${{ steps.should_run.outputs.should_run != 'false' && inputs.perf }}
        uses: actions/upload-artifact@v4
        with:
          name: perf
          path: |
            profiling/results

  benchmark-darwin:
    runs-on: [self-hosted, macos, bare-metal]

    steps:
      - name: Checkout benchmarking
        uses: actions/checkout@v4
      - name: Checkout CPython
        uses: actions/checkout@v4
        with:
          repository: ${{ inputs.fork }}/cpython
          path: cpython
          ref: ${{ inputs.ref }}
          fetch-depth: 50
      - name: Install dependencies from PyPI
        run: |
          python3 -m venv venv
          venv/bin/python -m pip install -r requirements.txt
      - name: Should we run?
        if: ${{ always() }}
        id: should_run
        run: |
          venv/bin/python -m bench_runner should_run ${{ inputs.force }} ${{ inputs.force }} ${{ inputs.ref }} ${{ inputs.machine }} false ${{ inputs.tier2 }} ${{ inputs.jit }} >> $GITHUB_OUTPUT
      - name: Checkout python-macrobenchmarks
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: pyston/python-macrobenchmarks
          path: pyston-benchmarks
          ref: ${{ env.PYSTON_BENCHMARKS_HASH }}
      - name: Checkout pyperformance
        uses: actions/checkout@v4
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        with:
          repository: python/pyperformance
          path: pyperformance
          ref: ${{ env.PYPERFORMANCE_HASH }}
      - name: Setup environment
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          echo "PKG_CONFIG_PATH=$(brew --prefix openssl@1.1)/lib/pkgconfig" >> $GITHUB_ENV
      - name: Build Python
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          cd cpython
          ./configure ${{ inputs.pgo == true && '--enable-optimizations --with-lto=yes' || '' }} ${{ inputs.tier2 == true && '--enable-experimental-jit=interpreter' || '' }} ${{ inputs.jit == true && '--enable-experimental-jit=yes' || '' }}
          make -j4
      # On macos ARM64, actions/setup-python isn't available, so we rely on a
      # pre-installed homebrew one, used through a venv
      - name: Install pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          venv/bin/python -m pip install --no-binary :all: ./pyperformance
      - name: Running pyperformance
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        run: |
          venv/bin/python -m bench_runner run_benchmarks benchmark cpython/python.exe ${{ inputs.fork }} ${{ inputs.ref }} ${{ inputs.benchmarks || 'all' }} --run_id ${{ github.run_id }} ${{ inputs.tier2 == true && '--flag PYTHON_UOPS' || '' }} ${{ inputs.jit == true && '--flag JIT' || '' }}
      # Pull again, since another job may have committed results in the meantime
      - name: Pull benchmarking
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run }}
        run: |
          # Another benchmarking task may have created results for the same
          # commit while the above was running. This "magic" incantation means
          # that any local results for this commit will override anything we
          # just pulled in in that case.
          git pull -s recursive -X ours --autostash --rebase
      - name: Add data to repo
        if: ${{ steps.should_run.outputs.should_run != 'false' && !inputs.dry_run }}
        uses: EndBug/add-and-commit@v9
        with:
          add: results
      - name: Upload artifacts
        if: ${{ steps.should_run.outputs.should_run != 'false' }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark
          path: |
            benchmark.json
          overwrite: true
